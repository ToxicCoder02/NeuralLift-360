# Training Settings
test: False  # test mode
save_mesh: False  # export an OBJ mesh with texture
eval_interval: 200  # evaluate on the valid set every interval epochs
seed: 12  # random seed for reproducibility
iters: 2500  # reduced number of training iterations (previously 5000)
lr: 1.0e-3  # initial learning rate
ckpt: 'latest'  # checkpoint to start training from
fp16: True  # use mixed precision (fp16) for training to save memory
backbone: 'grid_finite'  # NeRF backbone type (grid-based)

# NGP (Neural Graphics Primitives)
cuda_ray: False  # use CUDA raymarching (not supported here)
max_steps: 128  # further reduce max steps per ray (previously 256)
num_steps: 8  # further reduce number of steps per ray (previously 16)
upsample_steps: 8  # further reduce upsampled steps per ray (previously 16)
update_extra_interval: 4  # update extra status interval (when using cuda_ray)
max_ray_batch: 512  # reduced batch size for rays during inference (previously 1024)
albedo_iters: 50  # further reduce iterations for albedo shading (previously 100)
bg_radius: 1.4  # background model sphere radius
density_activation: 'exp'  # activation function for density
density_thresh: 0.1  # threshold for density grid
lambda_tv: 0  # total variation loss scaling
p_albedo: 0.25  # probability of using albedo for training
p_textureless: 0.5  # probability of textureless rendering
p_randbg: 0.75  # probability of using random backgrounds

# Residual Blob Settings
blob_density: 5  # maximum density for the density blob
blob_radius: 0.2  # radius for the density blob

# Camera Settings
w: 16  # further reduce render width for training (previously 32)
h: 16  # further reduce render height for training (previously 32)
normal_shape: 50  # reduced render height for normals (previously 100)
jitter_pose: True  # add jitter to sampled camera poses
bound: 1  # scene boundary box
dt_gamma: 0  # adaptive ray marching (set to 0 to disable)
min_near: 0.1  # minimum near distance for camera
radius_range: [0.4, 1.0]  # camera radius range during training
fovy_range: [40, 70]  # camera field of view range during training

# Directional Text Encoding
dir_text: True  # encode text with direction (front, side, back, etc.)
negative_dir_text: False  # use negative directional text
angle_overhead: 30  # overhead angle range
angle_front: 60  # front angle range

# Loss Function Settings
lambda_entropy: 0  # scale for alpha entropy loss
lambda_opacity: 1.0e-3  # scale for opacity loss
lambda_orient: 10  # scale for orientation loss
lambda_smooth: 0  # scale for smoothness loss
lambda_blur: 0  # scale for blur loss
distortion: 0.1  # distortion loss for MipNeRF360

# Test-time Rendering Settings
gui: False  # enable GUI for testing
W: 200  # further reduce test render width (previously 300)
H: 200  # further reduce test render height (previously 300)
fovy: 60  # field of view for the test camera

# Reference View Settings
mask_path: 'data/cabin4_centered_mask.png'  # path to mask
depth_path: 'data/cabin4_centered.npy'  # path to depth map
rgb_path: 'data/cabin4_centered.png'  # path to RGB image
warmup_epoch: 1  # number of epochs for warmup
init_theta: 90  # initial theta for camera pose
init_radius: 0.4  # initial radius for camera
front_fov: 60  # field of view for the front view
clip_img_weight: 1  # weight for CLIP image loss
front_ratio: 0.02  # ratio for reference views during training
front_dist_amplify: 10  # amplification factor for distance
front_dsmooth_amplify: 5  # amplification factor for depth smoothness
ref_perturb_prob: 0.05  # probability of reference perturbation
ref_rgb_weight: 40  # weight for reference RGB loss

# Diffusion Model Settings
guidance: sd_clipguide  # guidance method using Stable Diffusion + CLIP
min_sd: 50  # minimum timestep for diffusion model
max_sd: 950  # maximum timestep for diffusion model
eta: 0.8  # classifier-free guidance scale
dataset: text2img  # dataset type
sd_name: runwayml/stable-diffusion-v1-5  # pre-trained Stable Diffusion model
